
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Regression Diagnostics</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        table, th, td {
            border: 1px solid black;
            border-collapse: collapse;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        .math {
            text-align: center;
            margin: 1em 0;
        }
    </style>
</head>
<body>
    <h1>Part 1: Exploratory Data Analysis</h1>
    <br>
    <h2>What are Descriptive Statistics?</h2>
    <p><p>Descriptive statistics are a set of techniques used to summarize and describe the main features of a dataset. They provide a way to present quantitative descriptions in a manageable form. Here are some key points about descriptive statistics:</p>

<ol>
<li><p><strong>Purpose</strong>: The main goal of descriptive statistics is to provide a clear summary of the data, making it easier to understand and interpret.</p></li>
<li><p><strong>Types of Descriptive Statistics</strong>:</p>

<ul>
<li><p><strong>Measures of Central Tendency</strong>: These statistics describe the center of a dataset. The most common measures are:</p>

<ul>
<li><strong>Mean</strong>: The average of all data points.</li>
<li><strong>Median</strong>: The middle value when the data points are arranged in order.</li>
<li><strong>Mode</strong>: The most frequently occurring value in the dataset.</li>
</ul></li>
<li><p><strong>Measures of Dispersion</strong>: These statistics describe the spread or variability of the data. Common measures include:</p>

<ul>
<li><strong>Range</strong>: The difference between the highest and lowest values.</li>
<li><strong>Variance</strong>: A measure of how much the data points differ from the mean.</li>
<li><strong>Standard Deviation</strong>: The square root of the variance, indicating how much the data points typically deviate from the mean.</li>
</ul></li>
</ul></li>
<li><p><strong>Data Visualization</strong>: Descriptive statistics often include graphical representations of data, such as:</p>

<ul>
<li>Histograms</li>
<li>Box plots</li>
<li>Bar charts
These visual tools help to quickly convey information about the distribution and characteristics of the data.</li>
</ul></li>
<li><p><strong>Applications</strong>: Descriptive statistics are widely used in various fields, including business, healthcare, social sciences, and education, to summarize data and make informed decisions.</p></li>
</ol>

<p>In summary, descriptive statistics provide a way to summarize and understand data through numerical measures and visual representations, making it easier to analyze and communicate findings.</p>
</p>
    <br>
    <h2>The Table of Descriptive Statistics</h2>

    <div>
        <table>
  <tr><td>       </td><td>   ant_exclusion </td><td>   stem_diameter </td><td>   height </td><td>     canopy </td><td>   dw_healthy </td><td>   dw_infect </td><td>   dw_total </td><td>   fw_pulb </td><td>   fw_seeds </td><td>   fw_total </td><td>   ab_fl_op </td><td>   ab_fl_cl </td><td>    ab_fl </td></tr>
  <tr><td> count </td><td>      120        </td><td>       120       </td><td> 120      </td><td> 120        </td><td>       120    </td><td>     120     </td><td>    120     </td><td>     120   </td><td>     120    </td><td>      120   </td><td>    120     </td><td>     120    </td><td>   120    </td></tr>
  <tr><td> mean  </td><td>        0.5      </td><td>        27.0966  </td><td> 293      </td><td>   0.329775 </td><td>      1054.78 </td><td>     149.55  </td><td>   1203.53  </td><td>   16239.7 </td><td>    5404.45 </td><td>    21617.6 </td><td>   1550.13  </td><td>    3910.92 </td><td>  5452.61 </td></tr>
  <tr><td> std   </td><td>        0.502096 </td><td>         5.30599 </td><td>  34.6056 </td><td>   0.173708 </td><td>       750.21 </td><td>     146.953 </td><td>    824.982 </td><td>   10796.7 </td><td>    3541.42 </td><td>    14236.2 </td><td>    904.836 </td><td>    2230.83 </td><td>  3106.13 </td></tr>
  <tr><td> min   </td><td>        0        </td><td>        15.7375  </td><td> 223.75   </td><td>   0.015    </td><td>         0    </td><td>       0     </td><td>      0     </td><td>       0   </td><td>       0    </td><td>        0   </td><td>    241     </td><td>     741    </td><td>  1114    </td></tr>
  <tr><td> 25%   </td><td>        0        </td><td>        23.375   </td><td> 270.375  </td><td>   0.18125  </td><td>       447.25 </td><td>      46.75  </td><td>    573.25  </td><td>    8083.5 </td><td>    2824.25 </td><td>    11107.2 </td><td>    905.25  </td><td>    2277.5  </td><td>  3178.25 </td></tr>
  <tr><td> 50%   </td><td>        0.5      </td><td>        26.7125  </td><td> 287.875  </td><td>   0.344167 </td><td>       939.5  </td><td>     116     </td><td>   1135     </td><td>   14767   </td><td>    4983    </td><td>    19292   </td><td>   1360     </td><td>    3440.5  </td><td>  4798    </td></tr>
  <tr><td> 75%   </td><td>        1        </td><td>        30.3656  </td><td> 313.188  </td><td>   0.449167 </td><td>      1536    </td><td>     216.5   </td><td>   1717.5   </td><td>   21869.2 </td><td>    7544.5  </td><td>    29008.2 </td><td>   2018.75  </td><td>    5053.25 </td><td>  7006.75 </td></tr>
  <tr><td> max   </td><td>        1        </td><td>        46.6     </td><td> 399.5    </td><td>   0.708333 </td><td>      3045    </td><td>     920     </td><td>   3500     </td><td>   60787   </td><td>   17025    </td><td>    77812   </td><td>   4369     </td><td>   12469    </td><td> 16501    </td></tr>
</table>
    </div>
    <p><p>Summary statistics provide a concise overview of the main characteristics of a dataset. They are used to describe and summarize the data in a way that is easy to understand and interpret. Here are the key components of summary statistics:</p>

<ol>
<li><p><strong>Measures of Central Tendency</strong>:</p>

<ul>
<li><strong>Mean</strong>: The average of all data points, calculated by summing all values and dividing by the number of observations.</li>
<li><strong>Median</strong>: The middle value when the data points are arranged in ascending order. It divides the dataset into two equal halves.</li>
<li><strong>Mode</strong>: The most frequently occurring value in the dataset.</li>
</ul></li>
<li><p><strong>Measures of Dispersion</strong>:</p>

<ul>
<li><strong>Range</strong>: The difference between the maximum and minimum values in the dataset.</li>
<li><strong>Variance</strong>: A measure of how much the data points differ from the mean. It is the average of the squared differences from the mean.</li>
<li><strong>Standard Deviation</strong>: The square root of the variance, indicating how much the data points typically deviate from the mean.</li>
<li><strong>Interquartile Range (IQR)</strong>: The difference between the 75th percentile (Q3) and the 25th percentile (Q1), representing the spread of the middle 50% of the data.</li>
</ul></li>
<li><p><strong>Measures of Shape</strong>:</p>

<ul>
<li><strong>Skewness</strong>: A measure of the asymmetry of the data distribution. Positive skewness indicates a longer right tail, while negative skewness indicates a longer left tail.</li>
<li><strong>Kurtosis</strong>: A measure of the "tailedness" of the data distribution. High kurtosis indicates heavy tails, while low kurtosis indicates light tails.</li>
</ul></li>
<li><p><strong>Percentiles</strong>:</p>

<ul>
<li>Percentiles indicate the relative standing of a value within the dataset. For example, the 25th percentile (Q1) is the value below which 25% of the data falls.</li>
</ul></li>
</ol>

<p>Summary statistics provide a quick and informative snapshot of the data, helping to understand its central tendency, variability, and distribution shape. They are essential for data analysis and interpretation, allowing for comparisons between different datasets or groups.</p>
</p>
    <br>
    <h2>Visual Representation of the Data</h2>
    <h3>Histograms</h3>
    <p><p>I want to know more about histograms: 1. What are histograms? 2. What are the components of a histogram? 3. How do i interpret a histogram? 4. Provide histograms for every variable of the dataset. Use 'sns.histplot(kde=True,color='gray')' to display the histograms in a grid format.</p>
</p>
    <img src="desc_stat_hist.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the histograms for each variable in the dataset, displayed in a grid format. Each histogram includes a kernel density estimate (KDE) overlay to help visualize the distribution of the data.</p>
</p>
    <br>
    <h3>Boxplots</h3>
    <p><ol>
<li><p><strong>What are boxplots?</strong></p>

<ul>
<li>A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. Boxplots are useful for visualizing the central tendency, variability, and skewness of a dataset, as well as identifying outliers.</li>
</ul></li>
<li><p><strong>What are the components of a boxplot?</strong></p>

<ul>
<li><strong>Box</strong>: The central box represents the interquartile range (IQR), which contains the middle 50% of the data. The lower edge of the box is Q1 (25th percentile), and the upper edge is Q3 (75th percentile).</li>
<li><strong>Median Line</strong>: A line inside the box indicates the median (Q2) of the dataset.</li>
<li><strong>Whiskers</strong>: Lines extending from the box to the smallest and largest values within 1.5 times the IQR from the lower and upper quartiles, respectively. These lines are called whiskers.</li>
<li><strong>Outliers</strong>: Data points that fall outside the whiskers are considered outliers and are typically represented as individual points.</li>
</ul></li>
<li><p><strong>How do I interpret a boxplot?</strong></p>

<ul>
<li><strong>Central Tendency</strong>: The position of the median line within the box indicates the central tendency of the data. If the median is closer to Q1, the data may be skewed to the right; if it's closer to Q3, the data may be skewed to the left.</li>
<li><strong>Spread</strong>: The length of the box (IQR) shows the spread of the middle 50% of the data. A longer box indicates greater variability.</li>
<li><strong>Outliers</strong>: Individual points outside the whiskers indicate outliers, which may warrant further investigation.</li>
<li><strong>Comparison</strong>: Boxplots are useful for comparing distributions across different groups or categories.</li>
</ul></li>
<li><p><strong>Provide boxplots for every variable of the dataset. Use 'sns.boxplot(color='gray')' to display the boxplots in a grid format.</strong></p></li>
</ol>
</p>
    <img src="desc_stat_box.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the boxplots for each variable in the dataset, displayed in a grid format. Each boxplot provides a visual summary of the distribution, central tendency, and potential outliers for the respective variable.</p>
</p>
    <br>
    <h3>ECDF Plots</h3>
    <p><ol>
<li><p><strong>What are ECDF plots?</strong></p>

<ul>
<li>An ECDF (Empirical Cumulative Distribution Function) plot is a graphical representation of the cumulative distribution of a dataset. It shows the proportion or count of observations falling below each unique value in the dataset. ECDF plots are useful for visualizing the distribution of data and comparing different datasets.</li>
</ul></li>
<li><p><strong>What are the components of an ECDF plot?</strong></p>

<ul>
<li><strong>X-axis</strong>: Represents the data values.</li>
<li><strong>Y-axis</strong>: Represents the cumulative proportion or count of observations. The values range from 0 to 1 (or 0 to the total count if using counts).</li>
<li><strong>Step Function</strong>: The ECDF is a step function that increases at each data point, showing the cumulative proportion of data points up to that value.</li>
</ul></li>
<li><p><strong>How do I interpret an ECDF plot?</strong></p>

<ul>
<li><strong>Cumulative Proportion</strong>: The height of the step at each data point indicates the cumulative proportion of observations that are less than or equal to that value.</li>
<li><strong>Distribution Shape</strong>: The shape of the ECDF plot provides insights into the distribution of the data. A steep slope indicates a large number of observations in a small range, while a gradual slope indicates a more spread-out distribution.</li>
<li><strong>Comparisons</strong>: ECDF plots are useful for comparing the distributions of different datasets. Overlapping ECDFs suggest similar distributions, while differences in the plots indicate variations in the data.</li>
</ul></li>
<li><p><strong>Provide ECDF plots for every variable of the dataset. Use 'sns.ecdfplot(color = 'black')' to display the ECDF plots in a grid format.</strong></p></li>
</ol>
</p>
    <img src="desc_stat_ecdf.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the ECDF plots for each variable in the dataset, displayed in a grid format. Each plot shows the cumulative distribution of the data, providing insights into the distribution and allowing for easy comparison between different variables.</p>
</p>
    <br>
    <h3>QQ-Plots</h3>
    <p><ol>
<li><p><strong>What are QQ plots?</strong></p>

<ul>
<li>A QQ (Quantile-Quantile) plot is a graphical tool to assess if a dataset follows a particular theoretical distribution, most commonly the normal distribution. It compares the quantiles of the dataset against the quantiles of the theoretical distribution. If the data follows the distribution, the points will approximately lie on a straight line.</li>
</ul></li>
<li><p><strong>What are the components of a QQ plot?</strong></p>

<ul>
<li><strong>X-axis</strong>: Represents the theoretical quantiles from the specified distribution (e.g., normal distribution).</li>
<li><strong>Y-axis</strong>: Represents the quantiles of the dataset being analyzed.</li>
<li><strong>Line of Best Fit</strong>: A reference line that represents where the points would lie if the data perfectly followed the theoretical distribution.</li>
</ul></li>
<li><p><strong>How do I interpret a QQ plot?</strong></p>

<ul>
<li><strong>Straight Line</strong>: If the points in the QQ plot lie close to the reference line, it suggests that the data follows the specified distribution.</li>
<li><strong>Deviations</strong>: Deviations from the line indicate departures from the distribution. For example, points curving away from the line at the ends suggest heavy tails, while an S-shape indicates skewness.</li>
<li><strong>Outliers</strong>: Points that are far from the line may indicate outliers in the data.</li>
</ul></li>
<li><p><strong>Provide QQ plots for every variable of the dataset. Use 'stats.probplot(dist='norm')' to display the QQ plots in a grid format.</strong></p></li>
</ol>
</p>
    <img src="desc_stat_qq.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the QQ plots for each variable in the dataset, displayed in a grid format. Each plot compares the quantiles of the dataset against the quantiles of a normal distribution, helping to assess how closely the data follows a normal distribution.</p>
</p>
    <br>
    <h1>Part 2: The Multiple Linear Regression Model</h1>
    <br>
    <h2>What is Multiple Linear Regression?</h2>
    <p>Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression, which involves only one independent variable, to include multiple predictors. The goal is to find the best-fitting linear equation that describes how the dependent variable changes with the independent variables.</p>

<h3>Model Representation</h3>

<p>The multiple linear regression model can be represented as:</p>

<p>\( 
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i 
\)</p>

<p>Where:</p>

<ul>
<li>\( y_i \) is the dependent variable for the \( i \)-th observation.</li>
<li>\( \beta_0 \) is the intercept.</li>
<li>\( \beta_1, \beta_2, \ldots, \beta_p \) are the coefficients for the independent variables.</li>
<li>\( x_{ij} \) represents the \( j \)-th independent variable for the \( i \)-th observation.</li>
<li>\( \epsilon_i \) is the error term for the \( i \)-th observation.</li>
</ul>

<h3>Ranges of \( i \) and \( j \)</h3>

<ul>
<li>\( i \) ranges from 1 to \( n \), where \( n \) is the number of observations in the dataset.</li>
<li>\( j \) ranges from 1 to \( p \), where \( p \) is the number of independent variables.</li>
</ul>

<h3>Assumptions of a (Classical) Linear Regression Model</h3>

<ol>
<li><p><strong>Linearity</strong>: The relationship between the dependent variable and the independent variables is linear. Mathematically, this means:</p>

<p>\(
E(y_i | x_{i1}, x_{i2}, \ldots, x_{ip}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}
\)</p></li>
<li><p><strong>Independence</strong>: The observations are independent of each other. This implies that the error terms (\( \epsilon_i \)) are uncorrelated:</p>

<p>\(
\text{Cov}(\epsilon_i, \epsilon_j) = 0 \quad \text{for} \quad i \neq j
\)</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of the error terms is constant across all levels of the independent variables. Mathematically:</p>

<p>\(
\text{Var}(\epsilon_i) = \sigma^2 \quad \text{for all} \quad i
\)</p></li>
<li><p><strong>Normality</strong>: The error terms are normally distributed. This can be expressed as:</p>

<p>\(
\epsilon_i \sim N(0, \sigma^2)
\)</p></li>
<li><p><strong>No Multicollinearity</strong>: The independent variables are not perfectly linearly related. This means that no independent variable is a perfect linear combination of the others.</p></li>
</ol>

<h3>Summary of Assumptions in Mathematical Form</h3>

<ul>
<li>Linearity: \( E(y_i | x_{ij}) = \beta_0 + \sum_{j=1}^{p} \beta_j x_{ij} \)</li>
<li>Independence: \( \text{Cov}(\epsilon_i, \epsilon_j) = 0 \) for \( i \neq j \)</li>
<li>Homoscedasticity: \( \text{Var}(\epsilon_i) = \sigma^2 \)</li>
<li>Normality: \( \epsilon_i \sim N(0, \sigma^2) \)</li>
<li>No Multicollinearity: No perfect linear relationship among \( x_{ij} \)</li>
</ul>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>What are Regression Diagnostics?</h2>
    <p><p>Regression diagnostics refer to the process of evaluating the validity and reliability of a regression model. This involves assessing whether the assumptions underlying the regression analysis are met and identifying any potential issues that could affect the model's performance. The goal of regression diagnostics is to ensure that the model provides an accurate and meaningful representation of the data.</p>

<p>Key aspects of regression diagnostics include:</p>

<ol>
<li><p><strong>Model Fit</strong>: Evaluating how well the regression model captures the relationship between the dependent and independent variables. This involves checking if the model accurately predicts the observed data.</p></li>
<li><p><strong>Assumption Checking</strong>: Ensuring that the assumptions of the regression model, such as linearity, independence, homoscedasticity, and normality, are satisfied. Violations of these assumptions can lead to biased or inefficient estimates.</p></li>
<li><p><strong>Influence and Leverage</strong>: Identifying data points that have a disproportionate impact on the model's estimates. These points can skew the results and lead to incorrect conclusions.</p></li>
<li><p><strong>Residual Analysis</strong>: Analyzing the differences between observed and predicted values (residuals) to detect patterns or anomalies that may indicate problems with the model.</p></li>
<li><p><strong>Model Robustness</strong>: Assessing the stability of the model's estimates under different conditions or subsets of the data. This helps determine if the model's conclusions are reliable.</p></li>
</ol>

<p>Overall, regression diagnostics are essential for validating the integrity of a regression model and ensuring that the results are trustworthy and applicable to the data being analyzed.</p>
</p>
    <br>
    <h2>Building the Model</h2>
    <p><p>The OLS regression model has been built using <code>stem_diameter</code> as the dependent variable and all remaining variables as independent variables.</p>
</p>
    <br>
    <h1>Part 3: Regression Diagnostics</h1>
    <br>
    <h2>Outliers</h2>
    <h3>Outliers</h3>

<p>Outliers are data points that deviate significantly from the other observations in a dataset. They can occur due to variability in the data, errors in data collection, or they may represent a unique case that is not typical of the rest of the data. Outliers can have a substantial impact on statistical analyses, including regression models, as they can skew results and lead to misleading conclusions.</p>

<h3>Studentized Residuals</h3>

<p>Studentized residuals are a type of standardized residual used in regression analysis to identify outliers. They are calculated by dividing the residuals (the difference between observed and predicted values) by an estimate of their standard deviation. This standardization allows for a more accurate assessment of whether a residual is unusually large or small.</p>

<p>The formula for calculating studentized residuals is:</p>

<p>\(
t_i = \frac{e_i}{s(e_i)}
\)</p>

<p>Where:</p>

<ul>
<li>\( t_i \) is the studentized residual for the \( i \)-th observation.</li>
<li>\( e_i \) is the residual for the \( i \)-th observation.</li>
<li>\( s(e_i) \) is the estimated standard deviation of the residual \( e_i \).</li>
</ul>

<h3>Identifying Outliers with Studentized Residuals</h3>

<p>A common threshold for identifying outliers using studentized residuals is 3. This threshold is based on the properties of the normal distribution, where approximately 99.7% of data points lie within three standard deviations of the mean. Observations with studentized residuals greater than 3 (in absolute value) are considered potential outliers.</p>

<h3>Plotting Studentized Residuals</h3>

<p>Let's plot the studentized residuals against the index to identify any potential outliers. We'll use a red dashed line to indicate the threshold of 3.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="out.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Plot</h3>

<p>The plot above shows the studentized residuals for each observation in the dataset. The red dashed lines represent the threshold of 3 and -3. Observations with studentized residuals beyond these thresholds are considered potential outliers.</p>

<ul>
<li><strong>Observation 27</strong> is identified as a potential outlier because its studentized residual exceeds the threshold of 3.</li>
</ul>

<h3>Threshold Explanation</h3>

<p>The threshold of 3 is based on the properties of the normal distribution. In a normal distribution, approximately 99.7% of the data points lie within three standard deviations of the mean. Therefore, a studentized residual greater than 3 (or less than -3) suggests that the observation is unusually far from the predicted value, indicating it may be an outlier.</p>

<h3>Should Regression Diagnostics be Repeated After Removing Potential Outliers?</h3>

<p>Yes, regression diagnostics should be repeated after removing potential outliers. Outliers can have a significant impact on the regression model, affecting the estimates of coefficients, standard errors, and overall model fit. By removing outliers and re-evaluating the model, you can ensure that the results are robust and not unduly influenced by anomalous data points.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>High-Leverage Points</h2>
    <h3>High-Leverage Points</h3>

<p>High-leverage points are observations in a dataset that have an unusually large influence on the estimation of the regression coefficients. These points are located far from the center of the independent variable space and can disproportionately affect the fit of the regression model. High-leverage points can be identified using the leverage value, which measures the influence of an observation on the fitted values.</p>

<p>The threshold for identifying high-leverage points is often set at \( \frac{2p}{n} \), where:</p>

<ul>
<li>\( p \) is the number of parameters in the model (including the intercept).</li>
<li>\( n \) is the number of observations.</li>
</ul>

<h3>Cook's Distance</h3>

<p>Cook's Distance is a measure used to identify influential data points in a regression analysis. It quantifies the effect of removing an observation on the estimated regression coefficients. A large Cook's Distance indicates that the observation has a significant impact on the model's estimates.</p>

<p>The formula for Cook's Distance is:</p>

<p>\(
D_i = \frac{(e_i^2 / p) \cdot h_{ii}}{(1 - h_{ii})^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( D_i \) is Cook's Distance for the \( i \)-th observation.</li>
<li>\( e_i \) is the residual for the \( i \)-th observation.</li>
<li>\( h_{ii} \) is the leverage of the \( i \)-th observation.</li>
<li>\( p \) is the number of parameters in the model.</li>
</ul>

<p>A common threshold for identifying influential points using Cook's Distance is \( \frac{4}{n} \).</p>

<h3>Identifying High-Leverage Points and Influential Observations</h3>

<p>Let's plot the leverage values and Cook's Distance to identify any high-leverage points and influential observations. We'll use the specified thresholds and annotate the plots accordingly.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="hlev1.png" alt="Descriptive Statistics" style="width:100%;;max-width: 1000px;">
    <br>
    <img src="hlev2.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Plots</h3>

<h4>Leverage vs Index Plot</h4>

<ul>
<li>The plot shows the leverage values for each observation in the dataset.</li>
<li>The red dashed line represents the leverage threshold of \( \frac{2p}{n} \).</li>
<li>Observations with leverage values above this threshold are considered high-leverage points.</li>
<li>In this plot, observations that exceed the threshold are marked with red stemlines and annotated with their index.</li>
</ul>

<h4>Cook's Distance vs Index Plot</h4>

<ul>
<li>The plot displays Cook's Distance for each observation.</li>
<li>The red dashed line indicates the Cook's Distance threshold of \( \frac{4}{n} \).</li>
<li>Observations with Cook's Distance values above this threshold are considered influential points.</li>
<li>In this plot, observations that exceed the threshold are marked with red stemlines and annotated with their index.</li>
</ul>

<p>These plots help identify observations that have a significant impact on the regression model, either through their leverage or their influence on the model's estimates.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Non-Linearity</h2>
    <p>Explain non-linearity to me. Then, explain the rainbow test to me and provide mathematical equations. I want to understand the basic idea of the rainbow test. Afterwards, tell me if non linearity if violated by using the rainbow test and the plot of residuals vs fitted values. Explain and interpret the plot. Additional infos on the plot: - use sns.residplot(lowess=True) and plt.scatter(predictions, residuals, color='black')</p>

    <h3>Non-Linearity</h3>

<p>Non-linearity refers to a situation where the relationship between the independent variables and the dependent variable in a regression model is not linear. In a linear regression model, it is assumed that changes in the independent variables lead to proportional changes in the dependent variable. However, if the true relationship is non-linear, this assumption is violated, and the model may not accurately capture the underlying pattern in the data.</p>

<h3>Rainbow Test</h3>

<p>The rainbow test is a statistical test used to detect non-linearity in a regression model. It assesses whether the linearity assumption holds by comparing the fit of the model over different segments of the data. The test involves splitting the data into two parts and comparing the fit of the model on these segments.</p>

<p>The basic idea of the rainbow test is to check if the model fits the data equally well across different segments. If the fit is significantly different, it suggests that the linearity assumption may be violated.</p>

<p>The test statistic for the rainbow test is calculated as follows:</p>

<ol>
<li>Fit the model to the entire dataset and calculate the residual sum of squares (RSS).</li>
<li>Split the data into two segments and fit the model to each segment separately.</li>
<li>Calculate the RSS for each segment.</li>
<li>Compare the RSS of the full model with the sum of the RSS from the segments using an F-test.</li>
</ol>

<p>The null hypothesis of the rainbow test is that the model fits the data equally well across all segments, indicating linearity.</p>

<h3>Checking Non-Linearity with the Rainbow Test and Residuals Plot</h3>

<p>Let's perform the rainbow test and plot the residuals vs. fitted values to assess non-linearity.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="nonlin.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Rainbow Test and Residuals Plot</h3>

<h4>Rainbow Test</h4>

<ul>
<li><strong>Test Statistic</strong>: 1.2048</li>
<li><strong>P-value</strong>: 0.2550</li>
</ul>

<p>The p-value from the rainbow test is greater than the common significance level (e.g., 0.05), indicating that there is no strong evidence to reject the null hypothesis. This suggests that the linearity assumption is not violated, and the model fits the data equally well across different segments.</p>

<h4>Residuals vs Fitted Values Plot</h4>

<ul>
<li>The plot shows the residuals (differences between observed and predicted values) against the fitted values.</li>
<li>The red line represents a lowess (locally weighted scatterplot smoothing) fit to the residuals, providing a visual indication of any systematic patterns.</li>
<li>Ideally, the residuals should be randomly scattered around the horizontal line at zero, indicating that the model captures the data's pattern well.</li>
</ul>

<p>In this plot, the residuals appear to be randomly scattered without any clear pattern, supporting the conclusion from the rainbow test that the linearity assumption is not violated.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Heteroscedasticity</h2>
    <h3>Heteroscedasticity</h3>

<p>Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables. In a well-fitted linear regression model, the residuals should have constant variance, a condition known as homoscedasticity. When this condition is violated, and the variance of the residuals changes with the level of the independent variables, it is termed heteroscedasticity.</p>

<p>Heteroscedasticity can lead to inefficient estimates and affect the validity of statistical tests, as it violates one of the key assumptions of the classical linear regression model. It can result in biased standard errors, which in turn affect hypothesis tests and confidence intervals.</p>

<h3>Breusch-Pagan Test</h3>

<p>The Breusch-Pagan test is a statistical test used to detect heteroscedasticity in a regression model. It assesses whether the variance of the residuals is related to the independent variables.</p>

<p>The basic idea of the Breusch-Pagan test is to regress the squared residuals from the original regression model on the independent variables. If the independent variables explain a significant portion of the variance in the squared residuals, it suggests the presence of heteroscedasticity.</p>

<p>The test involves the following steps:</p>

<ol>
<li>Fit the original regression model and obtain the residuals (\( e_i \)).</li>
<li>Calculate the squared residuals (\( e_i^2 \)).</li>
<li>Regress the squared residuals on the independent variables.</li>
<li><p>Calculate the test statistic:</p>

<p>\(
\text{BP} = n \cdot R^2
\)</p>

<p>Where:</p>

<ul>
<li>\( n \) is the number of observations.</li>
<li>\( R^2 \) is the coefficient of determination from the regression of squared residuals.</li>
</ul></li>
<li><p>The test statistic follows a chi-square distribution with degrees of freedom equal to the number of independent variables.</p></li>
</ol>

<h3>Checking Heteroscedasticity with the Breusch-Pagan Test and Scale-Location Plot</h3>

<p>Let's perform the Breusch-Pagan test and plot the scale-location plot to assess heteroscedasticity.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="hetero.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Breusch-Pagan Test and Scale-Location Plot</h3>

<h4>Breusch-Pagan Test</h4>

<ul>
<li><strong>Test Statistic</strong>: 19.685</li>
<li><strong>P-value</strong>: 0.0733</li>
</ul>

<p>The p-value from the Breusch-Pagan test is slightly above the common significance level (e.g., 0.05), suggesting that there is not strong evidence to reject the null hypothesis of homoscedasticity. This implies that heteroscedasticity may not be a significant issue in this model.</p>

<h4>Scale-Location Plot</h4>

<ul>
<li>The plot shows the square root of the standardized residuals against the fitted values.</li>
<li>The red line represents a lowess (locally weighted scatterplot smoothing) fit to the data, providing a visual indication of any systematic patterns.</li>
<li>Ideally, the points should be randomly scattered around a horizontal line, indicating constant variance of residuals across all levels of the fitted values.</li>
</ul>

<p>In this plot, the points appear to be relatively evenly spread, with no clear pattern or trend, supporting the conclusion from the Breusch-Pagan test that heteroscedasticity is not a major concern in this model.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Correlation of Error Terms</h2>
    <h3>Correlation of Error Terms</h3>

<p>In regression analysis, one of the key assumptions is that the error terms (residuals) are uncorrelated with each other. This means that the error for one observation should not be related to the error for another observation. When this assumption is violated, it is known as autocorrelation or serial correlation. Autocorrelation is common in time series data, where the error terms may be correlated over time.</p>

<p>Autocorrelation can lead to inefficient estimates and affect the validity of statistical tests, as it violates the assumption of independence of errors.</p>

<h3>Durbin-Watson Test</h3>

<p>The Durbin-Watson test is a statistical test used to detect the presence of autocorrelation in the residuals of a regression model. It specifically tests for first-order autocorrelation, where the error term for one observation is correlated with the error term for the next observation.</p>

<p>The test statistic is calculated as follows:</p>

<p>\(
DW = \frac{\sum_{t=2}^{n} (e_t - e_{t-1})^2}{\sum_{t=1}^{n} e_t^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( DW \) is the Durbin-Watson statistic.</li>
<li>\( e_t \) is the residual for the \( t \)-th observation.</li>
<li>\( n \) is the number of observations.</li>
</ul>

<p>The Durbin-Watson statistic ranges from 0 to 4:</p>

<ul>
<li>A value around 2 suggests no autocorrelation.</li>
<li>A value less than 2 indicates positive autocorrelation.</li>
<li>A value greater than 2 indicates negative autocorrelation.</li>
</ul>

<h3>Checking Correlation of Error Terms with the Durbin-Watson Test and Residuals Plot</h3>

<p>Let's perform the Durbin-Watson test and plot the residuals over time to assess autocorrelation.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="cor.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Durbin-Watson Test and Residuals Plot</h3>

<h4>Durbin-Watson Test</h4>

<ul>
<li><strong>Durbin-Watson Statistic</strong>: 1.441</li>
</ul>

<p>The Durbin-Watson statistic is slightly below the recommended range of 1.5 to 2.5, suggesting the presence of positive autocorrelation in the residuals. This indicates that the error terms for consecutive observations may be correlated.</p>

<h4>Residuals Over Time Plot</h4>

<ul>
<li>The plot shows the studentized residuals plotted over the index (which can represent time or order of observations).</li>
<li>Ideally, the residuals should appear as a random scatter around zero, with no discernible pattern or trend.</li>
</ul>

<p>In this plot, there may be some patterns or trends in the residuals, which could indicate autocorrelation. This visual observation, combined with the Durbin-Watson statistic, suggests that there may be some correlation of error terms in the model.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Normality of Residuals</h2>
    <h3>Normality of Residuals</h3>

<p>In regression analysis, one of the assumptions is that the residuals (errors) are normally distributed. This assumption is important because it underlies many statistical tests and confidence intervals used in regression analysis. If the residuals are normally distributed, it suggests that the model is well-specified and that the estimates of the coefficients are reliable.</p>

<h3>Shapiro-Wilk Test</h3>

<p>The Shapiro-Wilk test is a statistical test used to assess the normality of a dataset. It tests the null hypothesis that the data is normally distributed against the alternative hypothesis that it is not.</p>

<p>The basic idea of the Shapiro-Wilk test is to compare the order statistics of the sample to the expected order statistics of a normal distribution. The test statistic \( W \) is calculated as follows:</p>

<p>\(
W = \frac{\left( \sum_{i=1}^{n} a_i x_{(i)} \right)^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( W \) is the Shapiro-Wilk test statistic.</li>
<li>\( x_{(i)} \) are the ordered sample values.</li>
<li>\( \bar{x} \) is the sample mean.</li>
<li>\( a_i \) are constants derived from the expected values of the order statistics of a standard normal distribution.</li>
</ul>

<p>A small \( W \) value indicates a departure from normality. The test provides a p-value to help determine whether to reject the null hypothesis of normality.</p>

<h3>Checking Normality of Residuals with the Shapiro-Wilk Test and QQ Plot</h3>

<p>Let's perform the Shapiro-Wilk test and create a QQ plot of the standardized residuals to assess normality.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="normality.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of the Shapiro-Wilk Test and QQ Plot</h3>

<h4>Shapiro-Wilk Test</h4>

<ul>
<li><strong>Test Statistic</strong>: 0.985</li>
<li><strong>P-value</strong>: 0.199</li>
</ul>

<p>The p-value from the Shapiro-Wilk test is greater than the common significance level (e.g., 0.05), indicating that there is no strong evidence to reject the null hypothesis of normality. This suggests that the residuals are approximately normally distributed.</p>

<h4>QQ Plot of Standardized Residuals</h4>

<ul>
<li>The QQ plot compares the quantiles of the standardized residuals to the quantiles of a normal distribution.</li>
<li>If the residuals are normally distributed, the points should lie approximately along the reference line.</li>
</ul>

<p>In this plot, the points closely follow the reference line, supporting the conclusion from the Shapiro-Wilk test that the residuals are approximately normally distributed. This indicates that the normality assumption is not violated in this model.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Collinearity of Predictors</h2>
    <h3>Collinearity of Predictors</h3>

<p>Collinearity, or multicollinearity, refers to a situation in regression analysis where two or more predictor variables are highly correlated. This means that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. Collinearity can cause problems in estimating the coefficients of the regression model, leading to inflated standard errors, reduced statistical power, and difficulties in determining the individual effect of each predictor.</p>

<h3>Variance Inflation Factor (VIF)</h3>

<p>The Variance Inflation Factor (VIF) is a measure used to quantify the degree of multicollinearity in a set of multiple regression variables. It provides an index that measures how much the variance of an estimated regression coefficient increases when your predictors are correlated.</p>

<p>The basic idea of VIF is to assess how much the variance of a regression coefficient is inflated due to multicollinearity. The VIF for a predictor variable is calculated as follows:</p>

<p>\(
VIF_j = \frac{1}{1 - R_j^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( VIF_j \) is the Variance Inflation Factor for the \( j \)-th predictor.</li>
<li>\( R_j^2 \) is the coefficient of determination obtained by regressing the \( j \)-th predictor on all other predictors.</li>
</ul>

<p>A VIF value of 1 indicates no correlation between the \( j \)-th predictor and the other predictors, while a VIF value greater than 10 is often used as a threshold to indicate significant multicollinearity.</p>

<h3>Checking Collinearity with VIF and Correlation Matrix</h3>

<p>Let's calculate the VIF for each predictor and plot the correlation matrix to assess collinearity.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="coll.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Interpretation of VIF and Correlation Matrix</h3>

<h4>Variance Inflation Factor (VIF)</h4>

<p>The VIF values for the predictors are as follows:</p>

<ul>
<li><strong>VIF &gt; 10</strong>: Indicates significant multicollinearity. In this dataset, several variables have VIF values well above 10, suggesting that multicollinearity is a concern. For example, <code>dw_healthy</code>, <code>dw_total</code>, <code>fw_pulb</code>, <code>fw_total</code>, and others have extremely high VIF values, indicating that these variables are highly correlated with other predictors.</li>
</ul>

<h4>Correlation Matrix</h4>

<ul>
<li>The correlation matrix provides a visual representation of the pairwise correlations between variables.</li>
<li>The values range from -1 to 1, where values close to 1 or -1 indicate strong positive or negative correlations, respectively.</li>
<li>The heatmap uses colors to represent the strength of the correlations, with darker colors indicating stronger correlations.</li>
</ul>

<p>In the heatmap, you can observe strong correlations between several pairs of variables, which aligns with the high VIF values observed. This suggests that multicollinearity is present in the dataset, which could affect the stability and interpretability of the regression model coefficients.</p>

<h3>How to Read the Plot</h3>

<ul>
<li><strong>Diagonal</strong>: The diagonal of the correlation matrix is always 1, as each variable is perfectly correlated with itself.</li>
<li><strong>Off-diagonal</strong>: The off-diagonal elements represent the correlation between different pairs of variables. Look for values close to 1 or -1 to identify strong correlations.</li>
<li><strong>Color Intensity</strong>: The color intensity in the heatmap indicates the strength of the correlation, with darker colors representing stronger correlations.</li>
</ul>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Summary of Results</h2>
    <p>Here's a summary of the diagnostic results for the regression model:</p>

<ol>
<li><p><strong>Outliers</strong>:</p>

<ul>
<li>Identified using studentized residuals with a threshold of 3.</li>
<li>Observation 27 was identified as a potential outlier.</li>
</ul></li>
<li><p><strong>High-Leverage Points</strong>:</p>

<ul>
<li>Identified using leverage values with a threshold of \( \frac{2p}{n} \).</li>
<li>Several observations exceeded the threshold, indicating high leverage.</li>
</ul></li>
<li><p><strong>Non-Linearity</strong>:</p>

<ul>
<li>Assessed using the rainbow test and residuals vs. fitted values plot.</li>
<li>The rainbow test did not provide strong evidence of non-linearity, and the residuals plot supported this conclusion.</li>
</ul></li>
<li><p><strong>Heteroscedasticity</strong>:</p>

<ul>
<li>Assessed using the Breusch-Pagan test and scale-location plot.</li>
<li>The Breusch-Pagan test suggested that heteroscedasticity is not a major concern, and the scale-location plot supported this finding.</li>
</ul></li>
<li><p><strong>Correlation of Error Terms</strong>:</p>

<ul>
<li>Assessed using the Durbin-Watson test and residuals over time plot.</li>
<li>The Durbin-Watson statistic indicated potential positive autocorrelation, and the residuals plot suggested some patterns.</li>
</ul></li>
<li><p><strong>Normality of Residuals</strong>:</p>

<ul>
<li>Assessed using the Shapiro-Wilk test and QQ plot.</li>
<li>The Shapiro-Wilk test indicated that the residuals are approximately normally distributed, and the QQ plot supported this conclusion.</li>
</ul></li>
<li><p><strong>Collinearity of Predictors</strong>:</p>

<ul>
<li>Assessed using Variance Inflation Factor (VIF) and correlation matrix.</li>
<li>Several predictors had VIF values well above 10, indicating significant multicollinearity. The correlation matrix showed strong correlations between several pairs of variables.</li>
</ul></li>
</ol>

<p>Overall, the diagnostics suggest that while the model does not exhibit severe issues with non-linearity, heteroscedasticity, or normality of residuals, there are concerns with outliers, high-leverage points, autocorrelation, and multicollinearity that may need to be addressed to improve the model's reliability and interpretability.</p>

</body>
</html>
