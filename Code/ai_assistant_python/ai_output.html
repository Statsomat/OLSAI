
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Regression Diagnostics</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        table, th, td {
            border: 1px solid black;
            border-collapse: collapse;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        .math {
            text-align: center;
            margin: 1em 0;
        }
    </style>
</head>
<body>
    <h1>Part 1: Exploratory Data Analysis</h1>
    <br>
    <h2>What are Descriptive Statistics?</h2>
    <p><p>Descriptive statistics are a set of brief descriptive coefficients that summarize a given data set, which can be either a representation of the entire population or a sample of a population. These statistics are broken down into measures of central tendency and measures of variability (spread).</p>

<ol>
<li><p><strong>Measures of Central Tendency</strong>: These measures indicate the central point of a data set. They include:</p>

<ul>
<li><strong>Mean</strong>: The average of all data points.</li>
<li><strong>Median</strong>: The middle value when the data points are arranged in ascending order.</li>
<li><strong>Mode</strong>: The most frequently occurring value in the data set.</li>
</ul></li>
<li><p><strong>Measures of Variability (Spread)</strong>: These measures describe the dispersion within a data set. They include:</p>

<ul>
<li><strong>Range</strong>: The difference between the highest and lowest values.</li>
<li><strong>Variance</strong>: The average of the squared differences from the mean.</li>
<li><strong>Standard Deviation</strong>: The square root of the variance, representing the average distance of each data point from the mean.</li>
<li><strong>Interquartile Range (IQR)</strong>: The range within which the central 50% of the data points lie.</li>
</ul></li>
</ol>

<p>Descriptive statistics provide simple summaries about the sample and the measures. They form the basis of virtually every quantitative analysis of data. By using descriptive statistics, you can understand the basic features of the data and get a quick overview of the data set.</p>
</p>
    <br>
    <h2>The Table of Descriptive Statistics</h2>

    <div>
        <table>
  <tr><td>       </td><td>   ant_exclusion </td><td>   stem_diameter </td><td>   height </td><td>     canopy </td><td>   dw_healthy </td><td>   dw_infect </td><td>   dw_total </td><td>   fw_pulb </td><td>   fw_seeds </td><td>   fw_total </td><td>   ab_fl_op </td><td>   ab_fl_cl </td><td>    ab_fl </td></tr>
  <tr><td> count </td><td>      120        </td><td>       120       </td><td> 120      </td><td> 120        </td><td>       120    </td><td>     120     </td><td>    120     </td><td>     120   </td><td>     120    </td><td>      120   </td><td>    120     </td><td>     120    </td><td>   120    </td></tr>
  <tr><td> mean  </td><td>        0.5      </td><td>        27.0966  </td><td> 293      </td><td>   0.329775 </td><td>      1054.78 </td><td>     149.55  </td><td>   1203.53  </td><td>   16239.7 </td><td>    5404.45 </td><td>    21617.6 </td><td>   1550.13  </td><td>    3910.92 </td><td>  5452.61 </td></tr>
  <tr><td> std   </td><td>        0.502096 </td><td>         5.30599 </td><td>  34.6056 </td><td>   0.173708 </td><td>       750.21 </td><td>     146.953 </td><td>    824.982 </td><td>   10796.7 </td><td>    3541.42 </td><td>    14236.2 </td><td>    904.836 </td><td>    2230.83 </td><td>  3106.13 </td></tr>
  <tr><td> min   </td><td>        0        </td><td>        15.7375  </td><td> 223.75   </td><td>   0.015    </td><td>         0    </td><td>       0     </td><td>      0     </td><td>       0   </td><td>       0    </td><td>        0   </td><td>    241     </td><td>     741    </td><td>  1114    </td></tr>
  <tr><td> 25%   </td><td>        0        </td><td>        23.375   </td><td> 270.375  </td><td>   0.18125  </td><td>       447.25 </td><td>      46.75  </td><td>    573.25  </td><td>    8083.5 </td><td>    2824.25 </td><td>    11107.2 </td><td>    905.25  </td><td>    2277.5  </td><td>  3178.25 </td></tr>
  <tr><td> 50%   </td><td>        0.5      </td><td>        26.7125  </td><td> 287.875  </td><td>   0.344167 </td><td>       939.5  </td><td>     116     </td><td>   1135     </td><td>   14767   </td><td>    4983    </td><td>    19292   </td><td>   1360     </td><td>    3440.5  </td><td>  4798    </td></tr>
  <tr><td> 75%   </td><td>        1        </td><td>        30.3656  </td><td> 313.188  </td><td>   0.449167 </td><td>      1536    </td><td>     216.5   </td><td>   1717.5   </td><td>   21869.2 </td><td>    7544.5  </td><td>    29008.2 </td><td>   2018.75  </td><td>    5053.25 </td><td>  7006.75 </td></tr>
  <tr><td> max   </td><td>        1        </td><td>        46.6     </td><td> 399.5    </td><td>   0.708333 </td><td>      3045    </td><td>     920     </td><td>   3500     </td><td>   60787   </td><td>   17025    </td><td>    77812   </td><td>   4369     </td><td>   12469    </td><td> 16501    </td></tr>
</table>
    </div>
    <p><p>Let's break down the summary statistics for each column in the dataset:</p>

<ol>
<li><p><strong>Count</strong>: This represents the number of observations in each column. For all columns, the count is 120, indicating that there are 120 data points for each variable.</p></li>
<li><p><strong>Mean</strong>: This is the average value of each column. For example, the mean of the <code>stem_diameter</code> is approximately 27.10, which means that the average stem diameter in the dataset is 27.10 units.</p></li>
<li><p><strong>Standard Deviation (std)</strong>: This measures the amount of variation or dispersion in the dataset. A higher standard deviation indicates more spread out data. For instance, the <code>fw_pulb</code> has a standard deviation of 10796.7, suggesting a wide range of values around the mean.</p></li>
<li><p><strong>Minimum (min)</strong>: This is the smallest value in each column. For example, the minimum <code>height</code> is 223.75, indicating the shortest height recorded in the dataset.</p></li>
<li><p><strong>25th Percentile (25%)</strong>: Also known as the first quartile, this is the value below which 25% of the data falls. For <code>canopy</code>, 25% of the data is below 0.18125.</p></li>
<li><p><strong>Median (50%)</strong>: This is the middle value when the data is ordered. It is also known as the second quartile. For <code>dw_healthy</code>, the median is 939.5, meaning half of the data points are below this value.</p></li>
<li><p><strong>75th Percentile (75%)</strong>: Also known as the third quartile, this is the value below which 75% of the data falls. For <code>fw_seeds</code>, 75% of the data is below 7544.5.</p></li>
<li><p><strong>Maximum (max)</strong>: This is the largest value in each column. For example, the maximum <code>ab_fl</code> is 16501, indicating the highest value recorded for this variable.</p></li>
</ol>

<p>These summary statistics provide a quick overview of the dataset, helping to understand the central tendency, spread, and range of the data.</p>
</p>
    <br>
    <h2>Visual Representation of the Data</h2>
    <h3>Histograms</h3>
    <p><p>Let's go through your questions about histograms:</p>

<ol>
<li><p><strong>What are histograms?</strong></p>

<ul>
<li>Histograms are graphical representations of the distribution of numerical data. They are used to visualize the frequency distribution of a dataset by dividing the data into bins (or intervals) and counting the number of data points that fall into each bin. The height of each bar in the histogram represents the frequency of data points within that bin.</li>
</ul></li>
<li><p><strong>What are the components of a histogram?</strong></p>

<ul>
<li><strong>Bins</strong>: These are the intervals into which the data is divided. The width of the bins can affect the appearance of the histogram.</li>
<li><strong>Bars</strong>: Each bar represents the frequency of data points within a bin. The height of the bar corresponds to the number of data points in that bin.</li>
<li><strong>X-axis</strong>: This axis represents the range of data values divided into bins.</li>
<li><strong>Y-axis</strong>: This axis represents the frequency or count of data points within each bin.</li>
</ul></li>
<li><p><strong>How do I interpret a histogram?</strong></p>

<ul>
<li><strong>Shape</strong>: Look at the overall shape of the histogram. It can be symmetric, skewed left or right, uniform, or have multiple peaks.</li>
<li><strong>Center</strong>: Identify where the center of the data lies. This can give you an idea of the average value.</li>
<li><strong>Spread</strong>: Observe how spread out the data is. A wider spread indicates more variability.</li>
<li><strong>Outliers</strong>: Check for any bars that are isolated from the rest of the data, which may indicate outliers.</li>
</ul></li>
</ol>

<p>Now, let's create histograms for each variable in the dataset using the specified style.</p>
</p>
    <img src="desc_stat_hist.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the histograms for each variable in the dataset. Each plot shows the distribution of data points for a specific variable, with the bars representing the frequency of data points within each bin. The smooth curve overlaid on each histogram is the Kernel Density Estimate (KDE), which provides a continuous probability density function to help visualize the distribution's shape.</p>
</p>
    <br>
    <h3>Boxplots</h3>
    <p><p>Let's explore your questions about boxplots:</p>

<ol>
<li><p><strong>What are boxplots?</strong></p>

<ul>
<li>Boxplots, also known as box-and-whisker plots, are a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. They provide a visual summary of the central tendency, variability, and skewness of a dataset, as well as identifying potential outliers.</li>
</ul></li>
<li><p><strong>What are the components of a boxplot?</strong></p>

<ul>
<li><strong>Box</strong>: The box represents the interquartile range (IQR), which contains the middle 50% of the data. The bottom and top of the box are the first (Q1) and third quartiles (Q3), respectively.</li>
<li><strong>Median Line</strong>: A line inside the box indicates the median (Q2) of the data.</li>
<li><strong>Whiskers</strong>: Lines extending from the box to the smallest and largest values within 1.5 times the IQR from the quartiles. They represent the range of the data excluding outliers.</li>
<li><strong>Outliers</strong>: Data points outside the whiskers are considered outliers and are often plotted as individual points.</li>
</ul></li>
<li><p><strong>How do I interpret a boxplot?</strong></p>

<ul>
<li><strong>Center</strong>: The median line shows the central value of the data.</li>
<li><strong>Spread</strong>: The length of the box and whiskers indicates the variability of the data.</li>
<li><strong>Skewness</strong>: If the median is closer to the bottom or top of the box, the data may be skewed.</li>
<li><strong>Outliers</strong>: Points outside the whiskers are potential outliers, indicating unusual data points.</li>
</ul></li>
</ol>

<p>Now, let's create boxplots for each variable in the dataset using the specified style.</p>
</p>
    <img src="desc_stat_box.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the boxplots for each variable in the dataset. Each plot provides a visual summary of the data's distribution, highlighting the median, quartiles, and potential outliers. The gray boxes represent the interquartile range (IQR), with the line inside indicating the median. Whiskers extend to the smallest and largest values within 1.5 times the IQR, and any points outside this range are considered outliers.</p>
</p>
    <br>
    <h3>ECDF Plots</h3>
    <p><p>Let's explore your questions about ECDF plots:</p>

<ol>
<li><p><strong>What are ECDF plots?</strong></p>

<ul>
<li>ECDF stands for Empirical Cumulative Distribution Function. An ECDF plot is a graphical representation of the cumulative distribution of a dataset. It shows the proportion or count of observations falling below each unique value in the dataset. ECDF plots are useful for visualizing the distribution of data and comparing different datasets.</li>
</ul></li>
<li><p><strong>What are the components of an ECDF plot?</strong></p>

<ul>
<li><strong>X-axis</strong>: Represents the data values.</li>
<li><strong>Y-axis</strong>: Represents the cumulative proportion or count of observations. It ranges from 0 to 1 (or 0 to the total count if using counts).</li>
<li><strong>Steps</strong>: The plot consists of steps that increase at each data point, showing the cumulative proportion of data points up to that value.</li>
</ul></li>
<li><p><strong>How do I interpret an ECDF plot?</strong></p>

<ul>
<li><strong>Cumulative Proportion</strong>: The Y-axis value at any point on the ECDF plot indicates the proportion of data points less than or equal to the corresponding X-axis value.</li>
<li><strong>Distribution Shape</strong>: The shape of the ECDF plot can give insights into the distribution of the data. A steep slope indicates a large number of data points in a small range, while a gradual slope indicates a more spread-out distribution.</li>
<li><strong>Comparisons</strong>: ECDF plots can be used to compare the distributions of different datasets by plotting multiple ECDFs on the same graph.</li>
</ul></li>
</ol>

<p>Now, let's create ECDF plots for each variable in the dataset using the specified style.</p>
</p>
    <img src="desc_stat_ecdf.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the ECDF plots for each variable in the dataset. Each plot shows the cumulative distribution of data points, with the X-axis representing the data values and the Y-axis representing the cumulative proportion of observations. The steps in the plot indicate the cumulative proportion of data points up to each value, providing a visual representation of the data's distribution.</p>
</p>
    <br>
    <h3>QQ-Plots</h3>
    <p><p>Let's explore your questions about Q-Q plots:</p>

<ol>
<li><p><strong>What are Q-Q plots?</strong></p>

<ul>
<li>Q-Q (Quantile-Quantile) plots are graphical tools used to compare the distribution of a dataset to a theoretical distribution, such as the normal distribution. They help assess whether the data follows a particular distribution by plotting the quantiles of the dataset against the quantiles of the theoretical distribution.</li>
</ul></li>
<li><p><strong>What are the components of a Q-Q plot?</strong></p>

<ul>
<li><strong>X-axis</strong>: Represents the theoretical quantiles from the specified distribution (e.g., normal distribution).</li>
<li><strong>Y-axis</strong>: Represents the quantiles of the dataset being analyzed.</li>
<li><strong>Line</strong>: A reference line (usually a 45-degree line) that represents where the points would lie if the data perfectly followed the theoretical distribution.</li>
</ul></li>
<li><p><strong>How do I interpret a Q-Q plot?</strong></p>

<ul>
<li><strong>Alignment with the Line</strong>: If the data points closely follow the reference line, it suggests that the data distribution is similar to the theoretical distribution.</li>
<li><strong>Deviations from the Line</strong>: Systematic deviations from the line indicate departures from the theoretical distribution. For example, a curve away from the line at the ends suggests heavy tails, while a curve away in the middle suggests a different central tendency.</li>
<li><strong>Outliers</strong>: Points that are far from the line may indicate outliers or unusual data points.</li>
</ul></li>
</ol>

<p>Now, let's create Q-Q plots for each variable in the dataset using the specified method.</p>
</p>
    <img src="desc_stat_qq.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p><p>Here are the Q-Q plots for each variable in the dataset. Each plot compares the quantiles of the dataset to the quantiles of a normal distribution. The red line represents the theoretical distribution, and the blue points represent the data. If the data follows a normal distribution, the points should closely follow the red line. Deviations from this line indicate departures from normality, such as skewness or heavy tails.</p>
</p>
    <br>
    <h1>Part 2: The Multiple Linear Regression Model</h1>
    <br>
    <h2>What is Multiple Linear Regression?</h2>
    <p>Multiple linear regression is a statistical technique used to model the relationship between one dependent variable and two or more independent variables. It extends simple linear regression by allowing for multiple predictors, providing a way to understand how each independent variable contributes to the dependent variable.</p>

<h3>Notation and Model</h3>

<p>In multiple linear regression, the model can be expressed as:</p>

<p>\( 
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i 
\)</p>

<p>Where:</p>

<ul>
<li>\( y_i \) is the dependent variable for the \( i \)-th observation.</li>
<li>\( x_{ij} \) represents the \( j \)-th independent variable for the \( i \)-th observation.</li>
<li>\( \beta_0 \) is the intercept.</li>
<li>\( \beta_1, \beta_2, \ldots, \beta_p \) are the coefficients for the independent variables.</li>
<li>\( \epsilon_i \) is the error term for the \( i \)-th observation.</li>
</ul>

<h3>Ranges of \( i \) and \( j \)</h3>

<ul>
<li>\( i \) ranges from 1 to \( n \), where \( n \) is the number of observations in the dataset.</li>
<li>\( j \) ranges from 1 to \( p \), where \( p \) is the number of independent variables.</li>
</ul>

<h3>Assumptions of the Classical Linear Regression Model</h3>

<ol>
<li><p><strong>Linearity</strong>: The relationship between the dependent variable and the independent variables is linear. Mathematically, this is expressed as:</p>

<p>\(
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i
\)</p></li>
<li><p><strong>Independence</strong>: The residuals (errors) are independent. This means that the error term \( \epsilon_i \) for one observation is not correlated with the error term \( \epsilon_j \) for another observation.</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of the residuals is constant across all levels of the independent variables. Mathematically, this is expressed as:</p>

<p>\(
\text{Var}(\epsilon_i) = \sigma^2 \quad \text{for all } i
\)</p></li>
<li><p><strong>Normality</strong>: The residuals are normally distributed. This assumption is important for hypothesis testing and constructing confidence intervals. Mathematically, this is expressed as:</p>

<p>\(
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
\)</p></li>
<li><p><strong>No Multicollinearity</strong>: The independent variables are not perfectly correlated with each other. This means that no independent variable is a perfect linear function of one or more other independent variables.</p></li>
</ol>

<h3>Summary of Assumptions in Mathematical Form</h3>

<ul>
<li>Linearity: \( y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i \)</li>
<li>Independence: \( \text{Cov}(\epsilon_i, \epsilon_j) = 0 \) for \( i \neq j \)</li>
<li>Homoscedasticity: \( \text{Var}(\epsilon_i) = \sigma^2 \)</li>
<li>Normality: \( \epsilon_i \sim \mathcal{N}(0, \sigma^2) \)</li>
<li>No Multicollinearity: \( \text{No perfect linear relationship among } x_{ij} \)</li>
</ul>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>What are Regression Diagnostics?</h2>
    <p><p>Regression diagnostics refer to the process of evaluating the validity and reliability of a regression model. This involves examining the assumptions underlying the model, assessing the quality of the fit, and identifying any potential issues that could affect the model's performance or interpretation. The goal of regression diagnostics is to ensure that the model accurately represents the data and provides meaningful insights.</p>

<p>Key aspects of regression diagnostics include:</p>

<ul>
<li><p><strong>Model Assumptions</strong>: Checking whether the assumptions of the regression model, such as linearity, independence, homoscedasticity, and normality, hold true for the data.</p></li>
<li><p><strong>Model Fit</strong>: Evaluating how well the model explains the variability in the dependent variable, often through measures of goodness-of-fit.</p></li>
<li><p><strong>Influential Data Points</strong>: Identifying observations that have a disproportionate impact on the model's estimates, which could skew results or lead to incorrect conclusions.</p></li>
<li><p><strong>Residual Analysis</strong>: Analyzing the residuals (differences between observed and predicted values) to detect patterns or anomalies that might indicate problems with the model.</p></li>
</ul>

<p>Overall, regression diagnostics help ensure that the model is robust, reliable, and suitable for making predictions or drawing conclusions from the data.</p>
</p>
    <br>
    <p><p>The Ordinary Least Squares (OLS) regression model has been successfully built using <code>stem_diameter</code> as the dependent variable and all other variables as independent variables. If you need further analysis or specific outputs from this model, feel free to ask!</p>
</p>
    <br>
    <h1>Part 3: Regression Diagnostics</h1>
    <br>
    <h2>Outliers</h2>
    <p>Let's go through your questions step by step:</p>

<h3>Outliers</h3>

<p>Outliers are data points that significantly differ from other observations in a dataset. They can occur due to variability in the data, measurement errors, or other anomalies. Outliers can have a substantial impact on statistical analyses, including regression models, as they can skew results and affect the model's accuracy and reliability.</p>

<h3>Studentized Residuals</h3>

<p>Studentized residuals are a type of standardized residual used in regression analysis to identify outliers. They are calculated by dividing the residuals by an estimate of their standard deviation, which accounts for the variability of the residuals. This standardization allows for a more accurate comparison of residuals across different observations.</p>

<p>The formula for calculating studentized residuals is:</p>

<p>\(
t_i = \frac{e_i}{s(e_i)}
\)</p>

<p>Where:</p>

<ul>
<li>\( t_i \) is the studentized residual for the \( i \)-th observation.</li>
<li>\( e_i \) is the residual for the \( i \)-th observation (the difference between the observed and predicted values).</li>
<li>\( s(e_i) \) is the estimated standard deviation of the residual \( e_i \).</li>
</ul>

<h3>Identifying Outliers with Studentized Residuals</h3>

<p>A common threshold for identifying outliers using studentized residuals is 3. This threshold is based on the properties of the normal distribution, where approximately 99.7% of data points lie within three standard deviations of the mean. Observations with studentized residuals greater than 3 (in absolute value) are considered potential outliers.</p>

<h3>Plotting Studentized Residuals</h3>

<p>Let's plot the studentized residuals against the index to identify any potential outliers. We'll use a red dashed line to indicate the threshold of 3 and annotate any outliers.</p>

<h3>Should Regression Diagnostics be Repeated?</h3>

<p>Yes, regression diagnostics should be repeated after removing potential outliers. Outliers can significantly affect the model's assumptions and fit, so it's important to reassess the model to ensure its validity and reliability after any modifications.</p>

<p>Let's create the plot to identify the outlier.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="out.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p>The plot above shows the studentized residuals for each observation in the dataset. The red dashed lines represent the threshold of \(\pm 3\). Observations with studentized residuals beyond these lines are considered potential outliers.</p>

<p>In this case, the observation with index 27 is identified as an outlier, as its studentized residual exceeds the threshold.</p>

<h3>Explanation of the Threshold</h3>

<p>The threshold of 3 is based on the properties of the normal distribution. In a normal distribution, approximately 99.7% of the data points lie within three standard deviations of the mean. Therefore, any data point with a studentized residual greater than 3 (in absolute value) is considered unusual and potentially an outlier.</p>

<h3>Should Regression Diagnostics be Repeated?</h3>

<p>Yes, after identifying and potentially removing outliers, it's important to repeat regression diagnostics. This ensures that the model's assumptions are still valid and that the model provides a reliable fit to the data without the influence of outliers.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>High-Leverage Points</h2>
    <p>Let's go through your questions step by step:</p>

<h3>High-Leverage Points</h3>

<p>High-leverage points are observations in a dataset that have an unusual combination of independent variable values. These points can exert significant influence on the regression model's fit, potentially skewing the results. High-leverage points are identified by their leverage value, which measures the influence of an observation on the fitted values.</p>

<p>The threshold for identifying high-leverage points is often set at \( \frac{2p}{n} \), where:</p>

<ul>
<li>\( p \) is the number of parameters in the model (including the intercept).</li>
<li>\( n \) is the number of observations.</li>
</ul>

<h3>Cook's Distance</h3>

<p>Cook's distance is a measure used to identify influential data points in a regression model. It quantifies the effect of removing an observation on the estimated regression coefficients. A large Cook's distance indicates that the observation has a significant impact on the model's fit.</p>

<p>The formula for Cook's distance is:</p>

<p>\(
D_i = \frac{\sum_{j=1}^{n} (\hat{y}_j - \hat{y}_{j(i)})^2}{p \cdot \text{MSE}}
\)</p>

<p>Where:</p>

<ul>
<li>\( D_i \) is Cook's distance for the \( i \)-th observation.</li>
<li>\( \hat{y}_j \) is the predicted value for the \( j \)-th observation using all data.</li>
<li>\( \hat{y}_{j(i)} \) is the predicted value for the \( j \)-th observation excluding the \( i \)-th observation.</li>
<li>\( p \) is the number of parameters in the model.</li>
<li>\(\text{MSE}\) is the mean squared error of the model.</li>
</ul>

<p>A common threshold for identifying influential points using Cook's distance is \( \frac{4}{n} \).</p>

<h3>Plotting Leverage and Cook's Distance</h3>

<p>Let's create the plots to identify high-leverage points and influential observations using the specified thresholds.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="hlev1.png" alt="Descriptive Statistics" style="width:100%;;max-width: 1000px;">
    <br>
    <img src="hlev2.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <p>The plots above show the leverage and Cook's distance for each observation in the dataset:</p>

<h3>Leverage vs Index Plot</h3>

<ul>
<li><strong>Black Stemlines</strong>: Represent the leverage values for each observation.</li>
<li><strong>Red Dashed Line</strong>: Indicates the leverage threshold of \(\frac{2p}{n}\).</li>
<li><strong>Red Stemlines and Annotations</strong>: Highlight observations with leverage values above the threshold, indicating high-leverage points. The indices of these points are annotated in red.</li>
</ul>

<h3>Cook's Distance vs Index Plot</h3>

<ul>
<li><strong>Black Stemlines</strong>: Represent Cook's distance for each observation.</li>
<li><strong>Red Dashed Line</strong>: Indicates the Cook's distance threshold of \(\frac{4}{n}\).</li>
<li><strong>Red Stemlines and Annotations</strong>: Highlight observations with Cook's distance above the threshold, indicating influential points. The indices of these points are annotated in red.</li>
</ul>

<h3>Interpretation</h3>

<ul>
<li><p><strong>High-Leverage Points</strong>: Observations with indices [20, 22, 26, 30, 36, 37, 42, 58, 74, 76, 106] have leverage values above the threshold, suggesting they have a significant influence on the model due to their unusual combination of independent variable values.</p></li>
<li><p><strong>Influential Points</strong>: Observations with indices [12, 20, 22, 27, 34, 35, 36, 74, 76] have Cook's distance values above the threshold, indicating they have a substantial impact on the regression model's fit.</p></li>
</ul>

<p>These points should be carefully examined to understand their influence on the model and to decide whether any action, such as further investigation or removal, is necessary.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Non-Linearity</h2>
    <p>Let's go through your questions step by step:</p>

<h3>Non-Linearity</h3>

<p>Non-linearity refers to a situation where the relationship between the independent variables and the dependent variable in a regression model is not linear. In a linear regression model, we assume that changes in the independent variables lead to proportional changes in the dependent variable. However, if this assumption is violated, the model may not accurately capture the true relationship, leading to biased or misleading results.</p>

<h3>Rainbow Test</h3>

<p>The rainbow test is a statistical test used to detect non-linearity in a regression model. It assesses whether the linear model is an appropriate fit for the data by comparing the fit of the model on a subset of the data to the fit on the entire dataset. The test involves splitting the data into two parts and comparing the sum of squared residuals for each part.</p>

<p>The basic idea is to test whether the model's fit is consistent across different subsets of the data. If the fit is significantly different, it suggests that the linear model may not be appropriate.</p>

<p>The test statistic is calculated as:</p>

<p>\(
F = \frac{\text{SSR}_{\text{full}} - \text{SSR}_{\text{subset}}}{\text{SSR}_{\text{subset}} / (n - k)}
\)</p>

<p>Where:</p>

<ul>
<li>\( \text{SSR}_{\text{full}} \) is the sum of squared residuals for the full model.</li>
<li>\( \text{SSR}_{\text{subset}} \) is the sum of squared residuals for the subset model.</li>
<li>\( n \) is the number of observations.</li>
<li>\( k \) is the number of parameters in the model.</li>
</ul>

<p>The test follows an F-distribution, and a significant result indicates potential non-linearity.</p>

<h3>Checking Non-Linearity</h3>

<p>Let's perform the rainbow test and create a plot of residuals vs. fitted values to assess non-linearity. We'll use <code>sns.residplot</code> with <code>lowess=True</code> and <code>plt.scatter</code> for visualization.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="nonlin.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Rainbow Test Results</h3>

<ul>
<li><strong>Rainbow Statistic</strong>: \(1.2048\)</li>
<li><strong>P-value</strong>: \(0.255\)</li>
</ul>

<p>The p-value from the rainbow test is greater than the common significance level of 0.05, indicating that there is no strong evidence to suggest non-linearity in the relationship between the independent variables and the dependent variable. Thus, the linear model appears to be an appropriate fit for the data.</p>

<h3>Residuals vs Fitted Values Plot</h3>

<ul>
<li><strong>Red Line</strong>: Represents the smoothed trend of residuals using a lowess curve.</li>
<li><strong>Black Points</strong>: Represent the residuals plotted against the fitted values.</li>
</ul>

<h4>Interpretation</h4>

<ul>
<li><strong>Random Scatter</strong>: The plot shows a random scatter of residuals around the horizontal line at zero, which is a good sign. It suggests that the residuals are evenly distributed and that the linear model is capturing the relationship well.</li>
<li><strong>No Clear Pattern</strong>: The absence of a clear pattern or systematic structure in the residuals indicates that the assumption of linearity is not violated.</li>
</ul>

<p>Overall, both the rainbow test and the residuals vs fitted values plot suggest that the linearity assumption holds for this model.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Heteroscedasticity</h2>
    <p>Let's explore your questions about heteroscedasticity and the Breusch-Pagan test in detail:</p>

<h3>Heteroscedasticity</h3>

<p>Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables. In a well-behaved linear regression model, we assume homoscedasticity, meaning the residuals have constant variance. When this assumption is violated, it can lead to inefficient estimates and affect the validity of hypothesis tests.</p>

<p><strong>Effects of Heteroscedasticity:</strong></p>

<ul>
<li><strong>Inefficient Estimates</strong>: The ordinary least squares (OLS) estimates remain unbiased, but they are no longer efficient, meaning they do not have the minimum variance among all unbiased estimators.</li>
<li><strong>Invalid Hypothesis Tests</strong>: Standard errors may be biased, leading to incorrect conclusions in hypothesis testing.</li>
</ul>

<h3>Breusch-Pagan Test</h3>

<p>The Breusch-Pagan test is a statistical test used to detect heteroscedasticity in a regression model. It tests whether the variance of the residuals is dependent on the values of the independent variables.</p>

<p><strong>Basic Idea:</strong></p>

<ul>
<li>The test involves regressing the squared residuals from the original regression model on the independent variables.</li>
<li>If the independent variables explain a significant portion of the variance in the squared residuals, it suggests heteroscedasticity.</li>
</ul>

<p><strong>Mathematical Formulation:</strong></p>

<ol>
<li>Fit the original regression model and obtain the residuals (\(e_i\)).</li>
<li>Calculate the squared residuals (\(e_i^2\)).</li>
<li>Regress \(e_i^2\) on the independent variables.</li>
<li>Compute the test statistic:</li>
</ol>

<p>\(
BP = \frac{n \cdot R^2}{2}
\)</p>

<p>Where:</p>

<ul>
<li>\( n \) is the number of observations.</li>
<li>\( R^2 \) is the coefficient of determination from the auxiliary regression of squared residuals on the independent variables.</li>
</ul>

<p>The test statistic follows a chi-squared distribution with degrees of freedom equal to the number of independent variables. A significant result indicates the presence of heteroscedasticity.</p>

<h3>Checking Heteroscedasticity</h3>

<p>Let's perform the Breusch-Pagan test and create a scale-location plot to assess heteroscedasticity. We'll use <code>sns.regplot</code> with <code>lowess=True</code> and <code>plt.scatter</code> for visualization.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="hetero.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Breusch-Pagan Test Results</h3>

<ul>
<li><strong>Test Statistic</strong>: \(19.685\)</li>
<li><strong>P-value</strong>: \(0.073\)</li>
</ul>

<p>The p-value from the Breusch-Pagan test is slightly above the common significance level of 0.05, suggesting that there is not strong evidence of heteroscedasticity in the model. However, it is close enough to warrant a cautious interpretation.</p>

<h3>Scale-Location Plot</h3>

<ul>
<li><strong>Red Line</strong>: Represents the smoothed trend of the square root of standardized residuals using a lowess curve.</li>
<li><strong>Black Points</strong>: Represent the square root of standardized residuals plotted against the fitted values.</li>
</ul>

<h4>Interpretation</h4>

<ul>
<li><strong>Random Scatter</strong>: Ideally, the plot should show a random scatter of points around the red line, indicating homoscedasticity.</li>
<li><strong>Pattern or Trend</strong>: If there is a clear pattern or trend (e.g., a funnel shape), it suggests heteroscedasticity, where the variance of residuals changes with the fitted values.</li>
</ul>

<p>In this case, the plot does not show a strong pattern, supporting the Breusch-Pagan test result that there is no significant evidence of heteroscedasticity. However, given the p-value is close to 0.05, it may be beneficial to monitor this aspect in further analyses or consider robust standard errors to mitigate potential issues.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Correlation of Error Terms</h2>
    <p>Let's explore your questions about the correlation of error terms and the Durbin-Watson test:</p>

<h3>Correlation of Error Terms</h3>

<p>In regression analysis, one of the key assumptions is that the error terms (residuals) are uncorrelated with each other. This means that the error for one observation should not be related to the error for another observation. When this assumption is violated, it indicates autocorrelation, which can lead to inefficient estimates and affect the validity of hypothesis tests.</p>

<p><strong>Effects of Autocorrelation:</strong></p>

<ul>
<li><strong>Inefficient Estimates</strong>: The ordinary least squares (OLS) estimates remain unbiased, but they are no longer efficient, meaning they do not have the minimum variance among all unbiased estimators.</li>
<li><strong>Invalid Hypothesis Tests</strong>: Standard errors may be biased, leading to incorrect conclusions in hypothesis testing.</li>
</ul>

<h3>Durbin-Watson Test</h3>

<p>The Durbin-Watson test is a statistical test used to detect the presence of autocorrelation in the residuals of a regression model. It specifically tests for first-order autocorrelation, where the residuals are correlated with their immediate predecessors.</p>

<p><strong>Basic Idea:</strong></p>

<ul>
<li>The test statistic is calculated by comparing the differences between consecutive residuals.</li>
</ul>

<p><strong>Mathematical Formulation:</strong>
The Durbin-Watson statistic is calculated as:</p>

<p>\(
DW = \frac{\sum_{t=2}^{n} (e_t - e_{t-1})^2}{\sum_{t=1}^{n} e_t^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( e_t \) is the residual at time \( t \).</li>
<li>\( n \) is the number of observations.</li>
</ul>

<p>The Durbin-Watson statistic ranges from 0 to 4:</p>

<ul>
<li>A value around 2 suggests no autocorrelation.</li>
<li>A value less than 2 indicates positive autocorrelation.</li>
<li>A value greater than 2 indicates negative autocorrelation.</li>
</ul>

<h3>Checking for Autocorrelation</h3>

<p>Let's perform the Durbin-Watson test and create a plot of residuals over time to assess autocorrelation. We'll use <code>plt.plot</code> for visualization.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="cor.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Durbin-Watson Test Result</h3>

<ul>
<li><strong>Durbin-Watson Statistic</strong>: \(1.441\)</li>
</ul>

<p>The Durbin-Watson statistic is slightly below the recommended range of 1.5 to 2.5, suggesting potential positive autocorrelation in the residuals. This means that the residuals may be correlated with their immediate predecessors, which could affect the efficiency of the regression estimates.</p>

<h3>Residuals Over Time Plot</h3>

<ul>
<li><strong>Black Line</strong>: Represents the studentized residuals plotted over the observation index.</li>
</ul>

<h4>Interpretation</h4>

<ul>
<li><strong>Pattern in Residuals</strong>: If the plot shows a clear pattern or trend over time, it suggests autocorrelation. In this case, the plot may show some clustering or systematic variation, which aligns with the Durbin-Watson test result indicating potential positive autocorrelation.</li>
</ul>

<p>Overall, the Durbin-Watson test and the residuals over time plot suggest that there may be some correlation in the error terms, which could impact the regression model's assumptions and efficiency.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Normality of Residuals</h2>
    <p>Let's explore your questions about the normality of residuals and the Shapiro-Wilk test:</p>

<h3>Normality of Residuals</h3>

<p>In regression analysis, one of the assumptions is that the residuals (errors) are normally distributed. This assumption is important for hypothesis testing and constructing confidence intervals. If the residuals are not normally distributed, it can affect the validity of statistical inferences made from the model.</p>

<p><strong>Effects of Non-Normality:</strong></p>

<ul>
<li><strong>Invalid Hypothesis Tests</strong>: The results of hypothesis tests may not be reliable if the residuals deviate significantly from normality.</li>
<li><strong>Biased Confidence Intervals</strong>: Confidence intervals may not accurately reflect the true variability of the estimates.</li>
</ul>

<h3>Shapiro-Wilk Test</h3>

<p>The Shapiro-Wilk test is a statistical test used to assess the normality of a dataset. It tests the null hypothesis that the data is normally distributed.</p>

<p><strong>Basic Idea:</strong></p>

<ul>
<li>The test compares the order statistics of the data to the expected order statistics of a normal distribution.</li>
</ul>

<p><strong>Mathematical Formulation:</strong>
The Shapiro-Wilk test statistic \( W \) is calculated as:</p>

<p>\(
W = \frac{\left( \sum_{i=1}^{n} a_i x_{(i)} \right)^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( x_{(i)} \) are the ordered sample values.</li>
<li>\( \bar{x} \) is the sample mean.</li>
<li>\( a_i \) are constants derived from the expected values of the order statistics of a standard normal distribution.</li>
</ul>

<p>The test statistic \( W \) ranges from 0 to 1, where a value close to 1 indicates that the data is likely normally distributed. A significant result (low p-value) suggests non-normality.</p>

<h3>Checking Normality</h3>

<p>Let's perform the Shapiro-Wilk test and create a Q-Q plot of standardized residuals to assess normality. We'll use <code>stats.probplot</code> for visualization.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="normality.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Shapiro-Wilk Test Result</h3>

<ul>
<li><strong>Shapiro-Wilk Statistic</strong>: \(0.985\)</li>
<li><strong>P-value</strong>: \(0.199\)</li>
</ul>

<p>The p-value from the Shapiro-Wilk test is greater than the common significance level of 0.05, indicating that there is no strong evidence to suggest that the residuals deviate from normality. Thus, the assumption of normality for the residuals appears to hold.</p>

<h3>Q-Q Plot of Standardized Residuals</h3>

<ul>
<li><strong>Q-Q Plot</strong>: Compares the quantiles of the standardized residuals to the quantiles of a normal distribution.</li>
<li><strong>45-Degree Line</strong>: Represents where the points would lie if the residuals were perfectly normally distributed.</li>
</ul>

<h4>Interpretation</h4>

<ul>
<li><strong>Alignment with the Line</strong>: The points in the Q-Q plot closely follow the 45-degree line, suggesting that the residuals are approximately normally distributed.</li>
<li><strong>Deviations</strong>: Minor deviations from the line are expected, but significant departures would indicate non-normality.</li>
</ul>

<p>Overall, both the Shapiro-Wilk test and the Q-Q plot suggest that the residuals are normally distributed, supporting the assumption of normality in the regression model.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Collinearity of Predictors</h2>
    <p>Let's explore your questions about collinearity of predictors and the Variance Inflation Factor (VIF):</p>

<h3>Collinearity of Predictors</h3>

<p>Collinearity, or multicollinearity, occurs when two or more independent variables in a regression model are highly correlated. This means that one predictor can be linearly predicted from the others with a substantial degree of accuracy. Collinearity can lead to several issues in regression analysis:</p>

<ul>
<li><strong>Unstable Coefficient Estimates</strong>: The estimates of the regression coefficients can become highly sensitive to small changes in the model.</li>
<li><strong>Inflated Standard Errors</strong>: This can lead to wider confidence intervals and less reliable hypothesis tests.</li>
<li><strong>Difficulty in Assessing Individual Predictor Effects</strong>: It becomes challenging to determine the individual effect of each predictor on the dependent variable.</li>
</ul>

<h3>Variance Inflation Factor (VIF)</h3>

<p>The Variance Inflation Factor is a measure used to quantify the severity of multicollinearity in a regression model. It provides an index that measures how much the variance of an estimated regression coefficient increases due to collinearity.</p>

<p><strong>Basic Idea:</strong></p>

<ul>
<li>VIF assesses how much the variance of a regression coefficient is inflated due to multicollinearity.</li>
</ul>

<p><strong>Mathematical Formulation:</strong>
For a given predictor \( X_j \), the VIF is calculated as:</p>

<p>\(
\text{VIF}(X_j) = \frac{1}{1 - R_j^2}
\)</p>

<p>Where:</p>

<ul>
<li>\( R_j^2 \) is the coefficient of determination obtained by regressing \( X_j \) on all other predictors.</li>
</ul>

<p>A VIF value greater than 10 is often used as a threshold to indicate significant multicollinearity.</p>

<h3>Checking for Collinearity</h3>

<p>Let's calculate the VIF for each predictor and create a correlation matrix plot to assess collinearity. We'll use <code>sns.heatmap</code> for visualization.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <img src="coll.png" alt="Descriptive Statistics" style="width:100%;max-width: 1000px;">
    <h3>Variance Inflation Factor (VIF) Results</h3>

<p>The VIF values for each predictor are as follows:</p>

<ul>
<li><strong>dw_healthy</strong>: 5902.42</li>
<li><strong>dw_infect</strong>: 224.64</li>
<li><strong>dw_total</strong>: 7094.61</li>
<li><strong>fw_pulb</strong>: 5845.52</li>
<li><strong>fw_seeds</strong>: 662.17</li>
<li><strong>fw_total</strong>: 10295.27</li>
<li><strong>ab<em>fl</em>op</strong>: 243.00</li>
<li><strong>ab<em>fl</em>cl</strong>: 1175.46</li>
<li><strong>ab_fl</strong>: 2344.46</li>
</ul>

<p>A VIF value greater than 10 indicates significant multicollinearity. Here, several predictors have VIF values far exceeding this threshold, suggesting severe multicollinearity issues.</p>

<h3>Correlation Matrix Plot</h3>

<p>The correlation matrix plot visualizes the pairwise correlations between all variables in the dataset:</p>

<ul>
<li><strong>Color Scale</strong>: The color scale ranges from blue (negative correlation) to red (positive correlation), with white indicating no correlation.</li>
<li><strong>Annotations</strong>: The numbers in each cell represent the correlation coefficient between the corresponding pair of variables.</li>
</ul>

<h4>Interpretation</h4>

<ul>
<li><strong>High Correlations</strong>: The plot shows several high correlations between variables, which aligns with the high VIF values. This indicates that some predictors are highly correlated with each other, contributing to multicollinearity.</li>
<li><strong>Reading the Plot</strong>: Each cell in the plot represents the correlation between two variables. A value close to 1 or -1 indicates a strong positive or negative correlation, respectively.</li>
</ul>

<p>Overall, both the VIF results and the correlation matrix plot suggest that multicollinearity is a significant issue in this dataset, which could affect the reliability of the regression model's estimates.</p>

    <script>
        MathJax.typeset();  // Renders the LaTeX after the page loads
    </script>
    <br>
    <h2>Summary of Results</h2>
    <p>Here's a concise summary of the diagnostic results for the regression model:</p>

<h3>Outliers</h3>

<ul>
<li><strong>Observation Identified</strong>: Index 27 was identified as an outlier based on studentized residuals exceeding the threshold of 3.</li>
</ul>

<h3>High-Leverage Points</h3>

<ul>
<li><strong>Identified Points</strong>: Indices [20, 22, 26, 30, 36, 37, 42, 58, 74, 76, 106] were identified as high-leverage points using the threshold of \( \frac{2p}{n} \).</li>
</ul>

<h3>Non-Linearity</h3>

<ul>
<li><strong>Rainbow Test</strong>: P-value of 0.255 indicates no strong evidence of non-linearity.</li>
<li><strong>Residuals vs Fitted Values Plot</strong>: Showed a random scatter, supporting the linearity assumption.</li>
</ul>

<h3>Heteroscedasticity</h3>

<ul>
<li><strong>Breusch-Pagan Test</strong>: P-value of 0.073 suggests no strong evidence of heteroscedasticity, though it's close to the significance level.</li>
<li><strong>Scale-Location Plot</strong>: Did not show a strong pattern, supporting homoscedasticity.</li>
</ul>

<h3>Correlation of Error Terms</h3>

<ul>
<li><strong>Durbin-Watson Test</strong>: Statistic of 1.441, slightly below the recommended range, suggests potential positive autocorrelation.</li>
<li><strong>Residuals Over Time Plot</strong>: Indicated some clustering, aligning with the test result.</li>
</ul>

<h3>Normality of Residuals</h3>

<ul>
<li><strong>Shapiro-Wilk Test</strong>: P-value of 0.199 indicates no strong evidence against normality.</li>
<li><strong>Q-Q Plot</strong>: Residuals closely followed the 45-degree line, supporting normality.</li>
</ul>

<h3>Collinearity of Predictors</h3>

<ul>
<li><strong>Variance Inflation Factor (VIF)</strong>: Several predictors had VIF values far exceeding the threshold of 10, indicating severe multicollinearity.</li>
<li><strong>Correlation Matrix Plot</strong>: Showed high correlations between several predictors, confirming multicollinearity issues.</li>
</ul>

<p>In summary, while the model meets some assumptions like linearity and normality of residuals, there are significant concerns with multicollinearity and potential autocorrelation that need to be addressed to improve the model's reliability and interpretability.</p>

</body>
</html>
